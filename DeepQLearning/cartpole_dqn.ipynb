{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_6JEXNKfitfZ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "import tensorflow as tf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRlScV2Fi-sa",
        "outputId": "eb5b9d5a-573b-4769-afe2-064c61ed7724"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (4.12.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.discrete = discrete\n",
        "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        dtype = np.int8 if self.discrete else np.float32\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        # store one hot encoding of actions, if appropriate\n",
        "        if self.discrete:\n",
        "            actions = np.zeros(self.action_memory.shape[1])\n",
        "            actions[action] = 1.0\n",
        "            self.action_memory[index] = actions\n",
        "        else:\n",
        "            self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = 1 - done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal\n",
        "\n",
        "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
        "    model = Sequential([\n",
        "                Dense(fc1_dims, input_shape=(input_dims,)),\n",
        "                Activation('relu'),\n",
        "                Dense(fc2_dims),\n",
        "                Activation('relu'),\n",
        "                Dense(n_actions)])\n",
        "\n",
        "    model.compile(optimizer='Adam', loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "class DDQNAgent(object):\n",
        "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,\n",
        "                 input_dims, epsilon_dec=0.996,  epsilon_end=0.01,\n",
        "                 mem_size=1000000, fname='/ddqn_model.h5', replace_target=100):\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_dec = epsilon_dec\n",
        "        self.epsilon_min = epsilon_end\n",
        "        self.batch_size = batch_size\n",
        "        self.model_file = fname\n",
        "        self.replace_target = replace_target\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions,\n",
        "                                   discrete=True)\n",
        "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 256, 256)\n",
        "        self.q_target = build_dqn(alpha, n_actions, input_dims, 256, 256)\n",
        "        \n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = state[np.newaxis, :]\n",
        "        rand = np.random.random()\n",
        "        if rand < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            actions = self.q_eval.predict(state,verbose = 0)\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr > self.batch_size:\n",
        "            state, action, reward, new_state, done = \\\n",
        "                                          self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "            action_values = np.array(self.action_space, dtype=np.int8)\n",
        "            action_indices = np.dot(action, action_values)\n",
        "\n",
        "            q_next = self.q_target.predict(new_state,verbose = 0)\n",
        "            q_eval = self.q_eval.predict(new_state,verbose = 0)\n",
        "            q_pred = self.q_eval.predict(state,verbose = 0)\n",
        "\n",
        "            max_actions = np.argmax(q_eval, axis=1)\n",
        "\n",
        "            q_target = q_pred\n",
        "\n",
        "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "            q_target[batch_index, action_indices] = reward + \\\n",
        "                    self.gamma*q_next[batch_index, max_actions.astype(int)]*done\n",
        "\n",
        "            _ = self.q_eval.fit(state, q_target, verbose=0)\n",
        "\n",
        "            self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > \\\n",
        "                           self.epsilon_min else self.epsilon_min\n",
        "            if self.memory.mem_cntr % self.replace_target == 0:\n",
        "                self.update_network_parameters()\n",
        "\n",
        "    def update_network_parameters(self):\n",
        "        self.q_target.set_weights(self.q_eval.get_weights())\n"
      ],
      "metadata": {
        "id": "Le00A2Huiv-A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "ddqn_agent = DDQNAgent(alpha=0.0005, gamma=0.99, n_actions=2, epsilon=1.0,\n",
        "              batch_size=64, input_dims=4)\n",
        "n_games = 500\n",
        "ddqn_scores = []\n",
        "\n",
        "for i in range(n_games):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        action = ddqn_agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        ddqn_agent.remember(observation, action, reward, observation_, int(done))\n",
        "        observation = observation_\n",
        "        ddqn_agent.learn()\n",
        "\n",
        "\n",
        "    ddqn_scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(ddqn_scores[max(0, i-100):(i+1)])\n",
        "    print('episode: ', i,'score: %.2f' % score,\n",
        "          ' average score %.2f' % avg_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9mYT3Ari6n5",
        "outputId": "74e5c070-d6f6-4f76-b009-54778fe5217f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  0 score: 24.00  average score 24.00\n",
            "episode:  1 score: 44.00  average score 34.00\n",
            "episode:  2 score: 55.00  average score 41.00\n",
            "episode:  3 score: 11.00  average score 33.50\n",
            "episode:  4 score: 12.00  average score 29.20\n",
            "episode:  5 score: 14.00  average score 26.67\n",
            "episode:  6 score: 19.00  average score 25.57\n",
            "episode:  7 score: 14.00  average score 24.12\n",
            "episode:  8 score: 15.00  average score 23.11\n",
            "episode:  9 score: 9.00  average score 21.70\n",
            "episode:  10 score: 13.00  average score 20.91\n",
            "episode:  11 score: 11.00  average score 20.08\n",
            "episode:  12 score: 11.00  average score 19.38\n",
            "episode:  13 score: 17.00  average score 19.21\n",
            "episode:  14 score: 13.00  average score 18.80\n",
            "episode:  15 score: 10.00  average score 18.25\n",
            "episode:  16 score: 9.00  average score 17.71\n",
            "episode:  17 score: 25.00  average score 18.11\n",
            "episode:  18 score: 14.00  average score 17.89\n",
            "episode:  19 score: 12.00  average score 17.60\n",
            "episode:  20 score: 12.00  average score 17.33\n",
            "episode:  21 score: 46.00  average score 18.64\n",
            "episode:  22 score: 25.00  average score 18.91\n",
            "episode:  23 score: 30.00  average score 19.38\n",
            "episode:  24 score: 159.00  average score 24.96\n",
            "episode:  25 score: 61.00  average score 26.35\n",
            "episode:  26 score: 161.00  average score 31.33\n",
            "episode:  27 score: 253.00  average score 39.25\n",
            "episode:  28 score: 216.00  average score 45.34\n",
            "episode:  29 score: 246.00  average score 52.03\n",
            "episode:  30 score: 201.00  average score 56.84\n",
            "episode:  31 score: 155.00  average score 59.91\n",
            "episode:  32 score: 183.00  average score 63.64\n",
            "episode:  33 score: 204.00  average score 67.76\n",
            "episode:  34 score: 174.00  average score 70.80\n",
            "episode:  35 score: 179.00  average score 73.81\n",
            "episode:  36 score: 190.00  average score 76.95\n",
            "episode:  37 score: 60.00  average score 76.50\n",
            "episode:  38 score: 163.00  average score 78.72\n",
            "episode:  39 score: 63.00  average score 78.33\n",
            "episode:  40 score: 61.00  average score 77.90\n",
            "episode:  41 score: 77.00  average score 77.88\n",
            "episode:  42 score: 72.00  average score 77.74\n",
            "episode:  43 score: 160.00  average score 79.61\n",
            "episode:  44 score: 198.00  average score 82.24\n",
            "episode:  45 score: 61.00  average score 81.78\n",
            "episode:  46 score: 90.00  average score 81.96\n",
            "episode:  47 score: 90.00  average score 82.12\n",
            "episode:  48 score: 152.00  average score 83.55\n",
            "episode:  49 score: 167.00  average score 85.22\n",
            "episode:  50 score: 131.00  average score 86.12\n",
            "episode:  51 score: 130.00  average score 86.96\n",
            "episode:  52 score: 143.00  average score 88.02\n",
            "episode:  53 score: 138.00  average score 88.94\n",
            "episode:  54 score: 142.00  average score 89.91\n",
            "episode:  55 score: 142.00  average score 90.84\n",
            "episode:  56 score: 135.00  average score 91.61\n",
            "episode:  57 score: 155.00  average score 92.71\n",
            "episode:  58 score: 153.00  average score 93.73\n",
            "episode:  59 score: 130.00  average score 94.33\n",
            "episode:  60 score: 141.00  average score 95.10\n",
            "episode:  61 score: 202.00  average score 96.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d2GmncUjoGg",
        "outputId": "75764203-487d-4052-8e28-4d8d99b35d89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "5WhaqfNolL1A",
        "outputId": "c7d1315d-5dad-48b7-e386-d462637fef7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-71b08b1d4748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ddqn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFAp4dl4KyPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}